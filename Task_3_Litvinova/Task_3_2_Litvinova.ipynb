{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_3_2_Litvinova.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FYYgB0jU6O_7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import pandas as pd\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "zdOPLkqM_c0g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/bioresponse.csv')\n",
        "Y_data = df['Activity']\n",
        "X_data = df.drop(columns = ['Activity'])\n",
        "X_1 = X_data.to_numpy()\n",
        "Y_1 = Y_data.to_numpy()"
      ],
      "metadata": {
        "id": "lKv505JY7pLE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1./(1.+np.exp(-z))\n",
        "    \n",
        "    return s"
      ],
      "metadata": {
        "id": "jEXaAf9q-Er7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    w = np.zeros((dim,1))\n",
        "    b = 0.\n",
        "    \n",
        "    return w, b"
      ],
      "metadata": {
        "id": "RR6bOMj--Hof"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X_1, Y_1, test_size=0.25)"
      ],
      "metadata": {
        "id": "_JNsC9vc-Kks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size which equals the number of features\n",
        "    b -- bias, a scalar\n",
        "    X -- data \n",
        "    Y -- true \"label\" vector (containing 0 and 1) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    #print('number of objects = ',len(X))\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T,X)+b )                                 # compute activation\n",
        "    cost = -(1./m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A),axis=1)   # compute cost\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = (1./m)*np.dot(X,(A-Y).T)\n",
        "    db = (1./m)*np.sum(A-Y,axis=1)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost\n",
        "    "
      ],
      "metadata": {
        "id": "EKW1mS9K-OXz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b = initialize_with_zeros(X_train.shape[1])\n",
        "print(w.shape)\n",
        "print(X_train.T.shape)\n",
        "Y_train = np.array([y_train])\n",
        "print(Y_train.shape)\n",
        "grads, cost = propagate(w, b, X_train.T, Y_train)\n",
        "#print(shape)\n",
        "\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"# of objects \" + str(X_train.T.shape[1]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333nBtUW-WCp",
        "outputId": "069337e8-0ead-4e47-b2e8-4f74f5733e1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1776, 1)\n",
            "(1776, 2813)\n",
            "(1, 2813)\n",
            "dw = [[ 0.00039713]\n",
            " [-0.02883808]\n",
            " [-0.00869179]\n",
            " ...\n",
            " [-0.00088873]\n",
            " [-0.00444365]\n",
            " [-0.00124422]]\n",
            "db = [-0.03928191]\n",
            "# of objects 2813\n",
            "cost = [0.69314718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array \n",
        "    b -- bias, a scalar\n",
        "    X -- data \n",
        "    Y -- true \"label\" vector (containing 0 and 1), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "                \n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "        \n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        k = (random.randrange(w.shape[0]))\n",
        "        # update rule\n",
        "        w[k] -=learning_rate*dw[k]\n",
        "        b -=learning_rate*db\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 1000 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "6-3gfHDL-bUA"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b = initialize_with_zeros(X_train.shape[1])\n",
        "grads, cost = propagate(w, b, X_train.T, Y_train)\n",
        "\n",
        "\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"# of objects \" + str(X_train.T.shape[1]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlRlrNrLAz0n",
        "outputId": "49ef7e20-b02d-4158-c47d-3ac762c9ac89"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dw = [[ 0.00039713]\n",
            " [-0.02883808]\n",
            " [-0.00869179]\n",
            " ...\n",
            " [-0.00088873]\n",
            " [-0.00444365]\n",
            " [-0.00124422]]\n",
            "db = [-0.03928191]\n",
            "# of objects 2813\n",
            "cost = [0.69314718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X_train.T, y_train.T, num_iterations= 40000, learning_rate = 0.4, print_cost = True)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvGQiQU2BNBr",
        "outputId": "d51c20f0-4ec9-4d95-c923-17b031c43b5a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.679231\n",
            "Cost after iteration 1000: 0.667251\n",
            "Cost after iteration 2000: 0.654793\n",
            "Cost after iteration 3000: 0.647684\n",
            "Cost after iteration 4000: 0.640368\n",
            "Cost after iteration 5000: 0.630796\n",
            "Cost after iteration 6000: 0.625008\n",
            "Cost after iteration 7000: 0.620138\n",
            "Cost after iteration 8000: 0.614411\n",
            "Cost after iteration 9000: 0.609850\n",
            "Cost after iteration 10000: 0.604613\n",
            "Cost after iteration 11000: 0.601100\n",
            "Cost after iteration 12000: 0.597773\n",
            "Cost after iteration 13000: 0.591913\n",
            "Cost after iteration 14000: 0.589382\n",
            "Cost after iteration 15000: 0.584359\n",
            "Cost after iteration 16000: 0.580034\n",
            "Cost after iteration 17000: 0.576798\n",
            "Cost after iteration 18000: 0.574654\n",
            "Cost after iteration 19000: 0.570422\n",
            "Cost after iteration 20000: 0.566086\n",
            "Cost after iteration 21000: 0.563838\n",
            "Cost after iteration 22000: 0.558671\n",
            "Cost after iteration 23000: 0.555437\n",
            "Cost after iteration 24000: 0.551983\n",
            "Cost after iteration 25000: 0.548651\n",
            "Cost after iteration 26000: 0.546716\n",
            "Cost after iteration 27000: 0.543963\n",
            "Cost after iteration 28000: 0.541447\n",
            "Cost after iteration 29000: 0.539820\n",
            "Cost after iteration 30000: 0.538172\n",
            "Cost after iteration 31000: 0.535842\n",
            "Cost after iteration 32000: 0.533519\n",
            "Cost after iteration 33000: 0.531360\n",
            "Cost after iteration 34000: 0.530289\n",
            "Cost after iteration 35000: 0.528884\n",
            "Cost after iteration 36000: 0.526872\n",
            "Cost after iteration 37000: 0.525775\n",
            "Cost after iteration 38000: 0.523893\n",
            "Cost after iteration 39000: 0.522076\n",
            "w = [[-0.01262635]\n",
            " [ 0.02148629]\n",
            " [ 0.02450694]\n",
            " ...\n",
            " [-0.00485476]\n",
            " [ 0.00853265]\n",
            " [ 0.00103665]]\n",
            "b = [-0.60757396]\n",
            "dw = [[ 0.00045858]\n",
            " [-0.00208511]\n",
            " [-0.00130721]\n",
            " ...\n",
            " [ 0.00046807]\n",
            " [-0.00061152]\n",
            " [-0.00016974]]\n",
            "db = [6.3149389e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array \n",
        "    b -- bias, a scalar\n",
        "    X -- data \n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities \n",
        "    A = sigmoid(np.dot(w.T,X)+b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if (A[0,i]<=0.5):\n",
        "            Y_prediction[0][i]=0\n",
        "        else:\n",
        "            Y_prediction[0][i]=1\n",
        "    \n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "57fI5qvlEa-P"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function we've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array \n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array \n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "metadata": {
        "id": "qMdfOGqWEb7n"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = model(X_train.T, y_train.T, X_test.T, y_test.T, num_iterations = 40000, learning_rate = 0.4, print_cost = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_5CbPMHEg-K",
        "outputId": "b2c01173-b8cc-4480-d920-3f3326bd4658"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 1000: 0.672497\n",
            "Cost after iteration 2000: 0.653778\n",
            "Cost after iteration 3000: 0.640664\n",
            "Cost after iteration 4000: 0.628780\n",
            "Cost after iteration 5000: 0.622370\n",
            "Cost after iteration 6000: 0.617437\n",
            "Cost after iteration 7000: 0.611801\n",
            "Cost after iteration 8000: 0.606004\n",
            "Cost after iteration 9000: 0.598186\n",
            "Cost after iteration 10000: 0.591936\n",
            "Cost after iteration 11000: 0.586929\n",
            "Cost after iteration 12000: 0.581399\n",
            "Cost after iteration 13000: 0.578557\n",
            "Cost after iteration 14000: 0.575301\n",
            "Cost after iteration 15000: 0.572706\n",
            "Cost after iteration 16000: 0.569868\n",
            "Cost after iteration 17000: 0.567840\n",
            "Cost after iteration 18000: 0.563837\n",
            "Cost after iteration 19000: 0.560111\n",
            "Cost after iteration 20000: 0.558099\n",
            "Cost after iteration 21000: 0.554868\n",
            "Cost after iteration 22000: 0.552870\n",
            "Cost after iteration 23000: 0.549915\n",
            "Cost after iteration 24000: 0.548026\n",
            "Cost after iteration 25000: 0.546334\n",
            "Cost after iteration 26000: 0.544479\n",
            "Cost after iteration 27000: 0.542799\n",
            "Cost after iteration 28000: 0.541456\n",
            "Cost after iteration 29000: 0.539991\n",
            "Cost after iteration 30000: 0.538808\n",
            "Cost after iteration 31000: 0.537439\n",
            "Cost after iteration 32000: 0.536164\n",
            "Cost after iteration 33000: 0.533752\n",
            "Cost after iteration 34000: 0.532489\n",
            "Cost after iteration 35000: 0.530121\n",
            "Cost after iteration 36000: 0.528819\n",
            "Cost after iteration 37000: 0.525006\n",
            "Cost after iteration 38000: 0.523743\n",
            "Cost after iteration 39000: 0.522822\n",
            "train accuracy: 76.43085673658017 %\n",
            "test accuracy: 74.09381663113007 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(d[\"num_iterations\"]/1000),d[\"costs\"])\n",
        "plt.xlabel('Iteration No.')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Dependence')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KDso5XoCHUm0",
        "outputId": "cbacf988-2953-4638-bbbe-bec3babb236f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On average, in the case of optimization by the SKD method, the error also decreases, although not so smoothly. And acuracy is a bit lower than in case of classic GD and test data\n"
      ],
      "metadata": {
        "id": "XCgOq2PRJPNu"
      }
    }
  ]
}