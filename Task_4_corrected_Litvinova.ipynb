{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_4_corrected_Litvinova.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiafRlQWFVWt",
        "outputId": "b5efbf7c-de16-4744-d041-5f5705fa6189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Downloadong the text\n"
      ],
      "metadata": {
        "id": "CatATj3LOQ98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('alice.txt', encoding='utf8').read()"
      ],
      "metadata": {
        "id": "nqrY41w1FWj4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Preproccessing"
      ],
      "metadata": {
        "id": "EcVU-9fKOU7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "bY2TNUbDmH18"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = f.replace(r'\\n', ' ')\n",
        "f = f.lower()\n",
        "tokens = pd.Series(word_tokenize(f))\n",
        "tokens = tokens.loc[~tokens.isin(stopwords.words('english'))]\n",
        "tokens = tokens.loc[~tokens.isin(list(string.punctuation + string.digits + \"'\" + '\"' + '”' + '“' + '’' + '‘'))]\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "tokens = tokens.apply(lambda word: lemmatizer.lemmatize(word))\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-7LUiY0jM-D",
        "outputId": "82e47bec-244d-4665-b260-258323f39b85"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             alice\n",
              "3         adventure\n",
              "5        wonderland\n",
              "7             lewis\n",
              "8           carroll\n",
              "            ...    \n",
              "34622    child-life\n",
              "34626         happy\n",
              "34627        summer\n",
              "34628           day\n",
              "34631           end\n",
              "Length: 12461, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into tokens\n",
        "tokens = pd.Series(word_tokenize(f))\n",
        "# deleting stopwords\n",
        "stop_words = stopwords.words(\"english\")\n",
        "tokens = tokens.loc[~tokens.isin(stop_words)]\n",
        "tokens = tokens.loc[~tokens.isin(list(string.punctuation + string.digits + \"'\" + '\"' + '”' + '“' + '’' + '‘'))]"
      ],
      "metadata": {
        "id": "gECQS72FFmDc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = tokens.apply(lambda w: lemmatizer.lemmatize(w))\n"
      ],
      "metadata": {
        "id": "fJFxGULjFxko"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grouping tokens in whole text into chapters and converting it to Dataframe\n",
        "chapters = []\n",
        "\n",
        "chapter_nmb = 0\n",
        "for word in tokens.iteritems():\n",
        "  if (word[1] == 'chapter'):\n",
        "    if chapter_nmb == 12:\n",
        "      chapter_nmb = 1\n",
        "    else:\n",
        "      chapter_nmb += 1\n",
        "  chapters.append({ 'chapter': chapter_nmb, 'word': word[1] })\n",
        "\n",
        "chapter_text = pd.DataFrame(chapters)"
      ],
      "metadata": {
        "id": "iIdroQlCi0vc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_text[\"chapter\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO6BAZeticvZ",
        "outputId": "c8ced215-a326-4f48-ccb5-c384e98c95e7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     1208\n",
              "6     1205\n",
              "8     1162\n",
              "9     1114\n",
              "7     1098\n",
              "10    1016\n",
              "5     1009\n",
              "12     988\n",
              "2      976\n",
              "1      970\n",
              "11     884\n",
              "3      821\n",
              "0       10\n",
              "Name: chapter, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Here we'r finding top-10 words in every chapter"
      ],
      "metadata": {
        "id": "oXwZlNZWOdXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topWords(df, chapter):\n",
        "  current_chapter = df[df.chapter == chapter]\n",
        "  unique_words = current_chapter[current_chapter.word != 'alice'].word.unique()\n",
        "\n",
        "  words_count = []\n",
        "\n",
        "  for w in unique_words:\n",
        "    words_count.append({\n",
        "      'word': w,\n",
        "      'count': current_chapter.loc[current_chapter.word == w].word.count()\n",
        "    })\n",
        "  \n",
        "  counted = pd.DataFrame(words_count)\n",
        "  return counted.sort_values(by=['count'], ascending=False).head(10)\n",
        "\n",
        "for i in range(12):\n",
        "  i +=1\n",
        "  print('Top 10 words in chapter ' + str(i))\n",
        "  print(topWords(chapter_text, i))\n",
        "  print('----------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOctWvODF17h",
        "outputId": "b7ff7d5d-1fc8-4018-8982-ee0c246579a1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words in chapter 1\n",
            "       word  count\n",
            "225  little     15\n",
            "93     like     11\n",
            "50      way     11\n",
            "84      see     10\n",
            "48    think      9\n",
            "296    door      9\n",
            "126     one      8\n",
            "61     time      8\n",
            "22    could      8\n",
            "159    said      8\n",
            "----------------------\n",
            "Top 10 words in chapter 2\n",
            "       word  count\n",
            "29   little     16\n",
            "366   mouse     16\n",
            "107    said     12\n",
            "34     dear     11\n",
            "52       go     11\n",
            "171   thing     10\n",
            "20     foot     10\n",
            "15     like      9\n",
            "2      pool      9\n",
            "27       oh      9\n",
            "----------------------\n",
            "Top 10 words in chapter 3\n",
            "      word  count\n",
            "55    said     34\n",
            "56   mouse     21\n",
            "169   dodo     12\n",
            "45    know     11\n",
            "201    one      8\n",
            "3     long      7\n",
            "65    soon      7\n",
            "10    bird      6\n",
            "37    lory      6\n",
            "25     dry      6\n",
            "----------------------\n",
            "Top 10 words in chapter 4\n",
            "        word  count\n",
            "4     little     24\n",
            "2     rabbit     16\n",
            "77      said     14\n",
            "198      one     14\n",
            "5       bill     13\n",
            "23       get      9\n",
            "15     heard      9\n",
            "25      sure      9\n",
            "224  thought      9\n",
            "168    quite      8\n",
            "----------------------\n",
            "Top 10 words in chapter 5\n",
            "            word  count\n",
            "16          said     52\n",
            "3    caterpillar     27\n",
            "382       pigeon     12\n",
            "367      serpent     12\n",
            "55          well     10\n",
            "64        little     10\n",
            "119       minute      8\n",
            "32         think      7\n",
            "52          size      7\n",
            "155        youth      6\n",
            "----------------------\n",
            "Top 10 words in chapter 6\n",
            "        word  count\n",
            "96      said     47\n",
            "225      cat     24\n",
            "34      like     16\n",
            "45    little     14\n",
            "60   duchess     14\n",
            "12   footman     12\n",
            "77      much     12\n",
            "209     baby     11\n",
            "21     would     11\n",
            "2        pig     10\n",
            "----------------------\n",
            "Top 10 words in chapter 7\n",
            "         word  count\n",
            "39       said     58\n",
            "11     hatter     33\n",
            "13   dormouse     27\n",
            "10       hare     21\n",
            "9       march     21\n",
            "68       time     17\n",
            "109     thing     12\n",
            "29        one     10\n",
            "113      well     10\n",
            "103      went     10\n",
            "----------------------\n",
            "Top 10 words in chapter 8\n",
            "         word  count\n",
            "35       said     42\n",
            "2       queen     38\n",
            "100      head     16\n",
            "144      king     14\n",
            "393       cat     11\n",
            "13      three     11\n",
            "314  hedgehog     10\n",
            "26        one     10\n",
            "21       went     10\n",
            "59        two      9\n",
            "----------------------\n",
            "Top 10 words in chapter 9\n",
            "        word  count\n",
            "11      said     57\n",
            "2       mock     27\n",
            "3     turtle     27\n",
            "312  gryphon     20\n",
            "12   duchess     19\n",
            "237    queen     14\n",
            "42      went     13\n",
            "190    never     11\n",
            "64    little     10\n",
            "140      say      9\n",
            "----------------------\n",
            "Top 10 words in chapter 10\n",
            "          word  count\n",
            "24        said     45\n",
            "25     gryphon     31\n",
            "5       turtle     30\n",
            "4         mock     28\n",
            "118      would     15\n",
            "2      lobster     14\n",
            "57       dance     13\n",
            "431       soup     11\n",
            "430  beautiful     11\n",
            "21       voice     10\n",
            "----------------------\n",
            "Top 10 words in chapter 11\n",
            "         word  count\n",
            "74       said     38\n",
            "4        king     26\n",
            "190    hatter     20\n",
            "37      court     15\n",
            "209  dormouse     13\n",
            "32        one     11\n",
            "5       queen      9\n",
            "188   witness      8\n",
            "53    thought      8\n",
            "30     rabbit      8\n",
            "----------------------\n",
            "Top 10 words in chapter 12\n",
            "       word  count\n",
            "56     said     50\n",
            "57     king     22\n",
            "52    would     12\n",
            "172   queen     10\n",
            "72   little     10\n",
            "89     jury      9\n",
            "87      one      8\n",
            "121  rabbit      8\n",
            "120   white      8\n",
            "20     head      8\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Here we are finding top-10 verbs in sent with Alce. We should split text not into chapters, but by symbols '.'"
      ],
      "metadata": {
        "id": "2zjppvMwOowA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = f.replace('”', '')\n",
        "f = f.replace('?', '')\n",
        "f = f.replace('!', '')\n",
        "sentences =  f.split('.')\n",
        "sentences_tokenized = [word_tokenize(sent) for sent in sentences]\n",
        "len(sentences_tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKVYCIS0F9LT",
        "outputId": "3679616e-7362-4f29-af3e-7563ea966535"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alice_list = []\n",
        "k = 0\n",
        "for sent in sentences_tokenized:\n",
        "  k = 0\n",
        "  for word in sent:\n",
        "    if word =='alice':\n",
        "      k +=1\n",
        "  if k>0:\n",
        "    alice_list.append(sent)\n",
        "\n",
        "len(alice_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOny4GIwGH04",
        "outputId": "8b323f8f-cf8c-4b15-a763-7a37a13583b1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "366"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter = []\n",
        "for sent in alice_list:\n",
        "  sent = [token for token in sent if token not in stop_words]\n",
        "  sent = [lemmatizer.lemmatize(token, pos = 'v') for token in sent]\n",
        "  postag = nltk.pos_tag(sent)\n",
        "  for w in postag:\n",
        "    if (w[1] == 'VB') or (w[1] == 'VBP'):\n",
        "      filter.append(w[0])"
      ],
      "metadata": {
        "id": "MX9y6wiaHpD-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y78vIWK5HrWZ",
        "outputId": "13fdca13-dd68-4228-88e2-8626e7e5b170"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['get', 'peep', 'think', 'consider', 'make', 'make', 'worth', 'get', 'run', 'think', 'say', 'think', 'occur', 'wonder', 'seem', 'see', 'burn', 'run', 'see', 'go', 'consider', 'go', 'like', 'think', 'stop', 'find', 'think', 'think', 'brave', 'think', 'say', 'let', 'see', 'say', 'wonder', 'think', 'say', 'begin', 'wonder', 'begin', 'get', 'go', 'say', 'see', 'go', 'say', 'turn', 'find', 'round', 'try', 'walk', 'wonder', 'get', 'make', 'belong', 'open', 'come', 'try', 'find', 'kneel', 'get', 'wander', 'get', 'go', 'think', 'little', 'see', 'begin', 'think', 'go', 'find', 'find', 'say', 'say', 'wise', 'go', 'find', 'say', 'shut', 'wait', 'see', 'end', 'know', 'say', 'go', 'find', 'decide', 'go', 'get', 'find', 'go', 'find', 'reach', 'see', 'try', 'try', 'use', 'advise', 'leave', 'give', 'follow', 'bring', 'remember', 'try', 'cheat', 'pretend', 'think', 'pretend', '’', 'leave', 'find', 'say', 'make', 'reach', 'make', 'get', 'happen', 'say', 'feel', 'find', 'happen', 'get', 'expect', 'seem', 'go', '’', 'look', 'seem', 'get', '’', 'manage', 'want', 'go', '’', 'give', 'lie', 'look', 'get', 'ashamed', 'say', 'say', 'go', 'go', 'tear', 'keep', 'begin', 'go', 'take', 'keep', 'go', 'go', 'change', 'say', 'say', 'begin', 'improve', 'seem', 'say', 'mabel', 'go', 'learn', 'make', 'stay', 'say', 'look', 'say', 'stay', 'dear', 'say', 'talk', 'say', 'find', 'think', 'say', 'come', 'go', 'find', 'dig', 'row', 'say', 'try', 'find', 'use', 'think', 'speak', 'think', 'know', 'think', 'right', 'do', 'remember', 'see', 'seem', 'wink', 'say', 'understand', 'think', 'daresay', 'come', 'knowledge', 'happen', '_you_', 'say', 'go', 'offend', 'talk', '’', 'let', 'say', 'you—are', 'go', 'know', 'throw', 'remember', 'know', 'say', 'say', 'go', 'make', 'go', '’', 'turn', 'swim', 'think', 'say', 'get', 'tell', 'understand', 'get', 'seem', 'find', 'know', 'say', 'know', 'allow', 'refuse', 'say', 'keep', 'catch', 'turn', 'say', 'seem', 'say', 'want', 'know', 'dodo', 'think', 'speak', 'seem', 'say', 'say', 'call', 'prize', 'get', 'go', 'turn', 'say', 'think', 'look', 'think', 'say', 'take', 'look', 'know', 'say', 'add', 'offend', 'mine', 'say', '_is_', 'say', 'look', 'call', 'keep', 'say', 'let', 'go', 'say', 'think', 'say', 'get', 'think', 'say', 'make', 'look', 'come', 'join', 'please', 'walk', 'know', 'say', 'get', 'canary', '“', 'come', '’', 'leave', 'seem', 'see', 'begin', 'send', 'go', 'lose', 'fur', 'get', 'wonder', 'seem', 'go', 'call', 'run', 'fetch', 'fan', 'explain', 'make', 'seem', 'say', 'go', 'send', 'happen', 'get', 'come', 'get', 'see', 'go', 'let', 'stop', 'find', 'take', 'go', 'grow', 'seem', 'get', 'wonder', 'think', '“', 'grow', 'think', '_never_', 'get', 'learn', '’', 'know', 'shake', 'forget', 'try', 'attempt', 'say', 'go', 'think', 'wait', 'make', 'go', 'take', 'hear', 'make', 'think', 'want', 'get', '’', 'reach', 'hold', 'come', 'fancy—who', 'go', 'go', 'say', 'go', '’', 'get', 'say', 'go', 'tell', 'think', 'know—no', 'thank', 'tell', 'know', 'go', 'say', 'burn', 'say', 'call', '“', 'think', 'take', 'begin', 'say', 'begin', 'come', 'turn', 'make', 'appear', 'run', 'find', 'say', '“', 'grow', 'find', 'say', 'try', 'frighten', 'hungry', 'eat', 'know', 'hold', 'make', 'believe', 'keep', 'make', 'get', 'think', 'expect', 'run', 'run', 'till', 'make', 'run', 'till', '’', 'forget', 'get', 'let', 'see—how', 'eat', 'drink', 'see', 'look', 'take', 'think', 'change', 'say', 'see', 'confuse', 'find', 'say', 'turn', 'think', 'say', 'feel', 'say', 'know', 'feel', 'make', 'say', 'think', 'tell', 'think', 'seem', 'turn', '’', 'say', 'turn', 'say', 'wait', 'tell', 'puff', 'take', 'say', 'think', 'say', 'remember', 'used—and', 'keep', '“', 'remember', 'say', 'say', 'come', 'say', 'stand', 'injure', 'say', 'reply', 'know', 'say', 'say', 'remain', 'look', 'try', 'make', 'find', 'find', 'find', 'see', 'look', 'seem', 'leave', 'say', 'say', 'let', 'say', 'add', 'try', 'seem', 'say', 'try', 'go', 'think', 'say', 'say', 'look-out', 'wink', '“', 'say', 'begin', 'see', 'take', 'continue', 'raise', 'think', 'need', 'come', 'tell', 'say', 'see', 'try', 'say', 'remember', 'go', 'say', 'know', 'give', 'know', 'say', 'want', 'neck', 'keep', 'live', 'think', 'come', 'frighten', 'begin', 'go', 'head', 'run', 'hear', 'go', 'knock', 'say', 'get', 'sense', 'go', 'look', 'think', 'get', 'say', 'say', 'go', 'say', 'sneeze', 'tell', 'say', 'speak', 'say', 'say', 'take', 'go', 'know', 'cheshire', 'grin', 'know', 'know', 'say', 'feel', 'get', 'introduce', '_not_', 'say', 'show', 'make', 'see', 'take', 'turn', 'say', 'cook', 'see', 'take', 'seem', 'go', '“', 'twelve', 'say', 'abide', 'begin', 'give', 'know', 'keep', 'hear', 'enjoy', 'nurse', 'say', 'hold', '“', 'think', 'take', 'think', 'leave', 'leave', 'say', '’', 'face', 'see', 'doubt', '_very_', 'get', 'go', 'say', 'think', 'get', 'look', 'think', 'go', '“', 'say', '—so', 'deny', 'try', 'want', 'go', 'know', 'say', 'think', 'go', 'know', 'say', 'say', 'say', 'say', 'get', 'happen', 'say', 'come', 'expect', 'appear', 'minute', 'say', 'say', 'reply', 'wish', 'keep', 'make', 'see', 'think', 'grin', 'go', 'right', 'roof', 'think', 'say', 'sit', 'say', 'know', 'say', 'look', 'make', 'say', 'fun', 'think', 'say', 'reply', 'know', 'say', 'see', 'see', 'say', 'add', 'get', 'say', 'add', 'seem', 'sleep', 'say', 'remember', 'say', 'take', 'look', 'shake', 'hold', 'consider', 'say', 'puzzle', 'say', 'turn', 'give', 'say', 'know', 'say', 'say', 'know', 'say', 'know', 'go', '“', 'give', 'know', 'hear', 'say', 'finish', 'say', 'savage', 'keep', 'say', 'happen', 'know', 'say', '“', 'live', 'say', 'take', 'do', 'know', 'live', 'like', 'puzzle', 'go', 'live', 'take', 'say', 'offend', 'take', 'ask', 'say', 'say', 'tea', 'begin', 'go', 'please', 'go', 'learn', 'know—', 'say', 'take', 'begin', 'say', 'think', 'draw', 'treacle', 'say', 'choose', 'let', 'dormouse', 'go', 'get', 'draw', 'begin', 'say', 'go', 'wake', 'go', 'know', 'say', 'see', 'say', 'say', 'bear', 'get', 'walk', 'take', 'call', 'try', 'go', 'say', 'think', 'go', 'come', 'go', 'say', 'elbow', 'begin', 'stand', 'check', 'look', 'tell', 'say', 'paint', 'look', 'see', 'hurry', 'go', 'lie', 'remember', 'use', 'think', 'lie', 'see', 'come', 'say', 'say', 'knave', 'say', 'turn', 'go', 'please', 'say', 'add', 'say', 'say', 'run', 'say', 'put', 'mean', 'roar', 'happen', 'say', 'say', 'say', 'say', 'think', 'think', 'see', 'live', 'make', 'find', 'succeed', 'get', 'tuck', 'get', 'straighten', 'go', 'help', 'burst', 'get', 'go', 'find', 'ridge', 'want', 'get', 'begin', 'know', 'happen', 'become', '’', 'leave', 'wonder', 'get', 'watch', 'make', 'say', 'talk', 'appear', 'begin', 'feel', 'think', 'play', 'begin', 'speak—and', 'seem', 'alive', 'arch', 'get', 'go', 'run', 'come', 'say', 'say', 'listen', 'go', 'win', '’', 'say', 'go', 'look', 'say', 'say', 'look', 'look', 'say', 'go', 'go', 'hear', 'seem', 'croquet', 'go', 'see', 'try', 'fly', 'catch', '’', 'think', 'arch', 'go', 'appear', 'find', 'make', 'say', 'think', 'say', 'think', 'dear', 'say', 'walk', 'find', 'think', 'make', 'keep', 'say', 'make', 'go', 'say', 'do', 'say', 'take', 'find', 'think', 'try', 'bite', 'reply', 'feel', 'say', 'say', 'say', 'seem', 'agree', 'say', '’', 'know', 'attend', 'say', 'follow', 'say', 'say', 'say', 'say', 'begin', 'feel', 'say', 'pig', 'die', 'stand', 'go', 'say', 'say', 'leave', 'take', 'leave', 'end', 'leave', 'leave', 'say', 'see', 'say', 'hear', 'say', 'say', 'tell', 'walk', 'say', 'go', 'see', 'leave', 'quite', 'go', 'say', 'say', 'come', 'say', 'think', 'go', 'go', 'come', 'hear', 'break', '“', 'see', 'begin', 'get', 'help', 'think', 'come', 'say', '’', 'teach', 'say', 'ashamed', 'add', 'say', 'go', 'go', 'believe', 'say', 'hold', 'speak', 'go', 'say', 'learn', 'say', 'want', 'hear', 'say', 'know', 'say', 'ask', 'say', 'learn', 'teach', 'drawl', 'say', 'say', 'hurry', 'exclaim', 'think', 'make', 'go', 'try', 'minute', 'go', 'live', 'say', 'introduce', 'say', 'say', '“', 'say', 'say', 'look', 'pretty', 'say', 'say', 'begin', 'wave', 'say', 'say', 'feel', 'say', 'see', 'check', 'believe', 'say', 'know', 'think', 'say', 'make', 'consider', 'make', 'say', 'run', 'say', 'keep', 'want', '’', 'say', 'go', 'say', 'say', 'come', 'go', 'say', 'mean', 'say', 'tell', 'say', 'go', 'begin', 'tell', 'think', 'make', 'think', 'say', '_ever_', 'happen', 'say', 'come', 'go', 'continue', 'follow', 'take', 'owl', 'permit', 'receive', 'conclude', 'explain', 'go', 'confuse', 'think', 'say', 'like', 'say', 'begin', 'wait', 'stoop', 'soup', 'give', 'come', 'take', 'answer', 'come', 'carry', 'make', 'get', 'do', 'think', 'seem', 'begin', 'pass', 'find', 'think', 'say', 'see', '“', 'begin', 'stop', 'look', 'make', 'see', 'look', 'write', 'make', 'know', 'ask', '_not_', 'stand', 'go', 'find', 'take', 'begin', 'grow', 'think', 'get', 'decide', 'remain', 'say', 'say', 'know', 'do', 'think', 'think', 'get', 'make', 'feel', 'see', 'like', '“', 'get', 'say', 'top', 'forget', 'head', 'remind', 'say', 'look', 'say', 'know', 'say', 'say', 'say', 'see', '’', 'think', 'look', 'say', 'go', 'say', 'say', 'say', 'say', 'let', '“', 'say', 'grow', '“', 'say', 'go', 'know', 'give', 'give', 'tarts', 'know—', 'go', 'say', 'say', 'say', 'say', 'grow', 'wake', 'dear', 'say', 'say', 'tell', 'remember', 'say', 'run', 'get', 'run', 'think', 'leave', 'think', 'till', 'begin', 'hear', 'see', 'keep', 'get', 'seem']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_verbs = []\n",
        "for w in filter:\n",
        "  counter = 1\n",
        "  for v in filter:\n",
        "    if w == v:\n",
        "      counter +=1\n",
        "      filter.remove(v)\n",
        "  top_verbs.append((w, counter))"
      ],
      "metadata": {
        "id": "LVnpIkhZJAxr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(top_verbs, key=lambda v: -v[1])[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNOSjz4xNhjM",
        "outputId": "fe5b40c4-7097-48fd-94c3-0236520d0545"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('say', 189),\n",
              " ('go', 85),\n",
              " ('think', 70),\n",
              " ('get', 48),\n",
              " ('know', 36),\n",
              " ('see', 34),\n",
              " ('make', 33),\n",
              " ('say', 33),\n",
              " ('find', 27),\n",
              " ('begin', 26)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}